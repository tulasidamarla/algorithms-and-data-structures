## Analysis of Algorithms

- To solve a specific problem p1, there might be many solutions possible. 
- Algorithm analysis helps to find efficient solution in terms of time and memory.
- Analysis of algorithms are expressed using Asymptotic notation. There are three types of asymptotic notations.
  - For which inputs algorithm takes the least amount of time to run (Best case)
  - For which inputs algorithm takes the longest amount of time to run (worst case)
  - For random inputs random running time of algorithm (Average case)
    - <b>Lower Bound <= Average Time <= Upper Bound </b>

## Asymptotic Notation
- Syntax for representing upper and lower bounds
- Upper and lower bounds need to be identified for best, worst and average cases
- "Asymptotic" means approaching a value or curve arbitrarily closely

### Big O Notation (upper bound function)
  - If f(n) denotes the running time(time complexity) of an algorithm , then O(g(n)) = f(n), there exists positive constants c and n0 such that 0 <= f(n) <= cg(n), for all n >= n0.
  - <i> Example </i>  

	  ```
	  f(n) = 3n + 2  
	  3n + 2 <= cn (This condition will be satisfied if c >= 4, we choose 4 because that is closest)  
	  3n + 2 <= 4n  
	  n >= 2
	  ```

  - Constant c is 4 and n0 is 2, which implies function 4n is always greater than 3n+2 when n >= 2.
  - <i>Conclusion</i> f(n) is always lessthan some constant multiplied by g(n), which describes upper bound of f(n).

### Big Omega  Notation Ω(lower bound function)
  - If f(n) denotes the time complexity of an algorithm, then Ω(g(n)) = f(n), there exists positive constants c and n0 such that 0 <= cg(n) <= f(n), for all n >= n0.
  - At larger values of n, the tighter lower bound of f(n) is g(n). For example, if f(n) = 100n<sup>2</sup> + 10n + 50, 	g(n) is Ω(n<sup>2</sup>).
  - <i>Example</i>
	 
 	```
	  f(n) = 3n+2  
	  3n + 2 >= cn (This condition will be satisfied if c = 3)  
	  3n + 2 >= 3n (This condition will be satisfied for all n values >= 1)
	 ```
	  
  - If c=3 and n0 = 1, all the conditions are satisfied for Big omega. i.e. f(n) = Ω(g(n))
  - <i>Conclusion</i> f(n) is always greater than some constant multiplied by g(n), which describes lower bound of f(n).

### Big Theta Notation (Order Function) 
  - If f(n) denotes the time complexity of an algorithm, then θ(g(n)) = f(n), there exists positive constants c1,c2,n0 such that 0 <= c1g(n) <= f(n) <= c2g(n), for all n >= n0.
  - Big theta is combination of Big oh and omega, but the function should satisfy the condition n >= n0.
  - From the above examples for f(n) = 3n+2, n0=2 for Big oh where as n0=1 for Big omega.

	  ```
 	  c1*n <= 3n + 2 <= c2*n, for all n > n0    
	  Solution 1 :: keep n0 as 1,but make c2 = 5, c1=3  
	  solution 2 :: n0 = 2, c1=3 and c2=4
 	  ```

<img src="big_oh.PNG" alt="Big Oh" align="middle" width="30%"><img src="big_omega.PNG" alt="Big Omega" align="middle" width="30%">
<img src="big_theta.PNG" alt="Big Theta" align="middle" width="30%">
<br>
<span style="display:inline-block;margin-right: 150px;" width="200px">Big Oh</span>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
<span style="display:inline-block;margin-right: 150px;" width="200px">Big Omega</span>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<span style="display:inline-block;" width="200px">Big Theta</span>

- <b><i>Note</b></i>
	- Big oh is used to find the worst case. In any scenario performance can't be worst than this for any value of n.
	- Big omega is used to find the best case. In any scenario performance can't be better than this for any value of n.
	- Big theta gives average case.
	- In practice Big omega is not very much useful. Big oh is used to find the worst case scenarios.

- <b><i>Example</i></b><br>
	- int[] array= {5,7,3,2,1,8,9};<br>
	- Finding an element from the array has best, worst and average cases possible.
	- For ex searching element 5, is found in the first attempt, which is the best case. It is represented as Ω(1)
	- For ex searching for element 9, requires scan of the entire array, which is the worst case. It is represented as O(n).
	- For average case it is θ(n/2), which is θ(n)
	
Time complexity Analysis
------------------------
There are two types of algorithms. They are iterative and recursive. Any function that is written using iterative algorithm can be written using recursive algorithm and vice versa. There is nothing like one is better than other. 

Time complexity for iterative functions is determined by the no of times an iteration is executed. Incase of recursive algorithm time complexity is calculated by recursive functions. If an algorithm has neither iteration nor recursive, time complexity such an algorithm is not dependent on input size n. It is constant.

Recursive algorithms analysis
-----------------------------
For recursive algorithms time complexity is calculated using recursive functions. There are multiple ways of solving recursive functions. 
1)Back substitution
2)recursion tree method
3)Masters theorm

We only discuss Back substitution. 

Back substitution
-----------------
For ex, we have an algorithm like this.
	
	public int calc(int n){
		if(n > 1){
			return calc(n-1);
		}else{
			return 1;
		}
	}
	
Lets say time taken for executing calc(n) is T(n). Then for if condition time taken will be constant. Time taken for executing calc(n-1) will be T(n-1). i.e.
	T(n) = 1 + T(n-1); // Constant can be represented as C or simply 1.
Now substitute T(n-1) in the equation gives,
	T(n-1) = 1 + T(n-2);
Now T(n) = 1 + 1 + T(n-2);
	T(n) = 2 + T(n-2);
After k iterations,
	T(n) = k + T(n-k);
As per the if condition, the equation is valid if n > 1. i.e. recusion stops when n=1; When n =1, T(n) = 1;	
if k becomes n-1 then n-k becomes 1. T(n) = (n-1) + T(n-(n-1))
	T(n) = n-1 + T(1);
	T(n) = n;

Example 2
---------
	T(n) = n + T(n-1) // assume n > 1 and T(1) = 1
Substitute n-1 for n, T(n-1) = (n-1) + T(n-2)
	T(n) = n + (n-1) + T(n-2)
This can be written as 
	T(n) = n + (n-1) + (n-2)+ ....+T(n-k)
As per the assumption,
	T(n) = n + (n-1) + (n-2)+ ....+1
	T(n) = n(n+1)/2;

Time complexity is O(n*n)
	
Space complexity
----------------	
Space complexity describes the amount of space an algorithm takes with respect to the input size. Sometimes we need to choose carefully between time and space complexities, because reducing time complexity may increase space complexity and vice versa.

Iterative algorithm
-------------------
Example
-------
	public void sort(int[] a){
		int size = a.length;
		for(int i=1; i<= size ; i++){
			//logic here
		}
	}

Space required to run the algorithm is for  size and i variable. It takes 2 variables. The space complexity is n+2. Because is n is input it is not considered. So, Space complexity is constant. i.e. O(1).

Example 2
---------
	public void sort(int[] a){
		int size = a.length;
		int[] b = new b[size];
		for(int i=1; i<= size ; i++){
			b[i] = a[i];
			//logic here
		}
	}
	
Space required here is for variables size, b and i. It means 1 + n + 1. i.e. n+2. Space complexity is O(n).

Example 3
---------
	public void sort(int[] a){
		int size = a.length;
		int[][] b = new b[size][size];
		for(int i=1; i<= size ; i++){
			for(int i=1; i<= size ; i++){
				b[i][j] = a[i];
				//logic here
			}	
		}
	}

Space required here is for variables size, b, i and j. It means 1 + n*n + 1 + 1. i.e. n*n+3. Space complexity is O(n^2).

Recursive algorithms
--------------------
Example
-------
	public int sort(int n){
		if(n > 1){
			sort(n-1);
			System.out.println(n);
		}
	}

Note: There are two types of recursion. Here sort() method is called at the begining of the method. Hence it is called head recursion. If sort() is called at the end then it is called tail recursion. If sort() is called in the middle it is called body recursion.

Recursion should stop using some condition. Here the condition is n > 1. This condition is called anchor condition or halting condition.

Space complexity of a recursive algorithm is calculated using method stack. For ex, The method sort() is called with n value 3. The method statck would be sort(3) --> sort(2) --> sort(1).

As there is no new variable used here, space complexity is not calculated using variables. It is calculated using stack size. For ex, imagine each method call takes a fixed space on the stack say k. so, For n value 3, The stack size is 3. It means order of n.

The space complexity of the above algorithm is O(n)

Example 2
---------
	public int sort(int n){
		if(n > 1){
			sort(n-1);
			System.out.println(n);
			sort(n-1);
		}
	}

Before finding Space complexity lets see the Time complexity first. Time complexity here is nothing but howmany times sort() method is executed. 

This program executes like a binary tree with left node gets executed first until the condition (n > 1) is satisfied. Then it prints the value of n, then right node is executed. Lets take n value of 2.

The tree would be like this. sort(2) --> sort(1) --> 2 --> sort(1)
o/p : 2 (sort(1) won't execute because of the exit condition n > 1)

No of times sort() method is called for n value of 2 is 3.

Lets see for n = 3;

The tree is sort(3) --> sort(2) --> sort(1) --> 2 --> sort(1) --> 3 --> sort(2) --> sort(1) --> 2 --> sort(1)

o/p: 232.

No of times sort() method is called for n value of 3 is 7.
No of times sort() method is called for n value of k is 2^k-1.

Space complexity is not the no of times a method is called, but howmany spaces occupied on method stack. It is evident that for n value 2, stack occupies 2 spaces. stack pops up when n value is 1 for left node and re uses the same when n value is 1 for right node. So, for n value of 2 , space complexity is 2. 

Conclusion: For the above problem , The space complexity is O(n).

Note: sort(2) called twice for sort(3) calculation and sor(1) is called 2 times for sor(2). Also, sort(1) called 4 times for sort(3). If the results are cached when they are called first time, the performace can be increased. This is also called dynamic programming.

Lets findout the time complexity. As we have already seen the no of times sort() method is called for a given input size 2, it is 2^n. which is nothing but time complexity. Anyway lets find that out using recursion method.

Lets say time taken for executing sort(n) is T(n). Then 
	T(n) = 2T(n-1) + 1;
	T(n-1) = 2T(n-2) + 1; //substitute this in above equation.
	T(n) = 2(2T(n-2)+1)+1;
		 = 2^2T(n-2) + 2 + 1;
	T(n) = 2^k(T(n-k)) + 2^(k-1) + 1;
Substitue k = n-1, to meet the anchor condition
	T(n) = 2^(n-1) + 2^(n-2) + ...+ 2^2+2^1;
	T(n) = 2^n;

Time complexity is O(2^n).

