Divide and Conquer
------------------
A divide-and-conquer algorithm is recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly.

The strategy is :

		dac(p){
			if(small(p)){
				solution(p);
			} else {
				divide p into p1,p2,p3.....pk
				apply dac(p1),dac(p2)....apply(dac(pk))
				combine(dac(p1),dac(p2),...dac(pk));
			}
		}

Common problems that can be solved using Divide and Conquer approach are
- Binary Search
- Find max and min
- Merge Sort
- Quick Sort
- Strassen's matrix multiplication

Divide and Conquer strategy is recursive. For this we should know how to find time complexity of recursive functions. For this we use <b>Recurrence Relation</b>

Recurrence Relation
--------------------
Problem 1
---------

		void test(n){
			if( n > 0) { // constant time c0
				System.out.println(n); // constant time c1
				test(n-1); // T(n-1)
			}
		}

T(n) = T(n-1) + (c0 + c1) 
<ins>T(n) = T(n-1) + 1 </ins> // we consider asymptotic notation. so c0 + c1 can be written as 1.

Finding Time complexity
-----------------------

Terminating condition for algorithm is n > 0, it means when n = 0, T(n) = 1.

		T(n) =  1 			when n = 0
		T(n) = T(n-1) + 1 	when n > 0
		
Solution:
		
		T(n) = T(n-1) + 1
		T(n) = T(n-2) + 2 // substitute the formula T(n-1) = T(n-2) + 1
		T(n) = T(n-3) + 3 // substitute the formula T(n-2) = T(n-3) + 1
		....
		T(n) = T(n-k) + k

		T(n) = T(n-n) + n // when k = n

So, time complexity of the problem is O(n).

Problem 2
---------

		void test(n){
			if( n > 0) { // constant time c0
				for(int i = 0; i < n ; i++){ // n times
					System.out.println(n); // constant time c1 * n
				}
				test(n-1); // T(n-1)
			}
		}

T(n) = T(n-1) + n + c<sub>1</sub>n + c<sub>0</sub>
T(n) = T(n-1) + (1 + c<sub>1</sub>)n + c<sub>1</sub>

<ins>T(n) = T(n-1) + n </ins> // Assympotatic notation for the above (1 + c<sub>1</sub>)n + c<sub>1</sub> becomes n.

Finding Time complexity
-----------------------

Terminating condition for algorithm is n > 0, it means when n = 0, T(n) = 1.

		T(n) =  1 			when n = 0
		T(n) = T(n-1) + n 	when n > 0
		
Solution:
		
		T(n) = T(n-1) + n
		T(n) = T(n-2) + (n-1) + n // substitute the formula T(n-1) = T(n-2) + (n-1)
		T(n) = T(n-3) + (n-2) + (n-1) + n // substitute the formula T(n-2) = T(n-3) + (n-2)
		....
		T(n) = T(n-k) + (n-k+1) + (n-k+2) + ....(n-2) + (n-1) + n

		T(n) = T(n-n) + 1 + 2 + 3 + ..... + (n-2) + (n-1) + n // when k = n
		
		T(n) = n(n+1)/2

So, time complexity of the problem is O(n<sup>2</sup>).

Problem 3
---------

		void test(n){
			if( n > 0) { // constant time c0
				for(int i = 0; i < n ; i*2){ // log(n) times
					System.out.println(n); // constant time c1 * log(n)
				}
				test(n-1); // T(n-1)
			}
		}

T(n) = T(n-1) + log(n) + c<sub>1</sub>log(n) + c<sub>0</sub>
T(n) = T(n-1) + (c<sub>0</sub> + c<sub>1</sub>)log(n) + c<sub>0</sub>

<ins>T(n) = T(n-1) + log(n) </ins> // Assympotatic notation for the above (c<sub>0</sub> + c<sub>1</sub>)log(n) + c<sub>0</sub> becomes log(n).

Finding Time complexity
-----------------------

Terminating condition for algorithm is n > 0, it means when n = 0, T(n) = 1.

		T(n) =  1 			when n = 0
		T(n) = T(n-1) + log(n) 	when n > 0
		
Solution:
		
		T(n) = T(n-1) + log(n)
		T(n) = T(n-2) + log(n-1) + log(n) // substitute the formula T(n-1) = T(n-2) + log(n-1)
		T(n) = T(n-3) + log(n-2) + log(n-1) + log(n) // substitute the formula T(n-2) = T(n-3) + log(n-2)
		....
		T(n) = T(n-k) + log(n-k+1) + log(n-k+2) + ....log(n-3) + log(n-2) + log(n-1) + log(n)

		T(n) = T(n-n) + log(1) + log(2) + log(3)....log(n-3) + log(n-2) + log(n-1) + log(n) // when k = n
		
		T(n) = 1 + log(1*2*3*...n) // using the formula log(a) + log(b) = log(ab)
		T(n) = 1 + log(n!) // The next higher order term of n! is n<sup>n</sup>
		T(n) = 1 + nlog(n)
		

So, time complexity of the problem is O(nlog(n)).
		
Problem 4
---------
		
		void test(n){
			if( n > 0) { // constant time c0
				System.out.println(n); // constant time c1 * log(n)
				test(n-1); // T(n-1)
				test(n-1); // T(n-1)
			}
		}

T(n) = 2T(n-1) + c<sub>1</sub> + c<sub>0</sub>

<ins>T(n) = 2T(n-1) + 1 // Assympotatic notation for the above c<sub>1</sub> + c<sub>0</sub> </ins> becomes 1.

Finding Time complexity
-----------------------

Terminating condition for algorithm is n > 0, it means when n = 0, T(n) = 1.

		T(n) =  1 			when n = 0
		T(n) = T(n-1) + log(n) 	when n > 0
		
Solution:
		
		T(n) = 2T(n-1) + 1
		T(n) = 2^2T(n-2) + 2 + 1 // substitute the formula T(n-1) = T(n-2) + log(n-1)
		T(n) = 2^3T(n-3) + 2^2 + 2 + 1 // substitute the formula T(n-2) = T(n-3) + log(n-2)
		....
		T(n) = 2^kT(n-k) + 2^(k-1) + 2^(k-2) + ....2^3 + 2^2 + 2^1 + 2^0

		T(n) = 2^n + 2^(n-1) + 2^(n-2) + ....2^3 + 2^2 + 2^1 + 2^0  // when k = n
		
		T(n) = 2^(n+1)-1 // using the formula a + ar^2 + ar^3 + ...ar^n-1 = a(r^n-1)/(r-1) where r = 2 and a =1 in this scenario

So, time complexity of the problem is O(2<sup>n</sup>).

Deriving masters theorm from the above solved problems
------------------------------------------------------

		T(n) = T(n-1) + 1		->	O(n)
		T(n) = T(n-1) + n		->	O(n^2)
		T(n) = T(n-1) + log(n)	->	O(nlog(n))
		T(n) = 2T(n-1)+ 1		-> 	O(2^n)
		T(n) = 3T(n-1)+ 1		-> 	O(3^n)	
		T(n) = 2T(n-1)+ n		-> 	O(n2^n)
Based on the above 3 problems we have solved already, we can find time complexity for the following problem:
		
		T(n) = T(n-2) + 1	->	n/2 (decrease by 2 means n/2) ≈ O(n)
		T(n) = T(n-100) + 1	->	n/100 ≈ O(n)
		T(n) = T(n-k) + 1 	->  n/k ≈ O(n) (at larger values of n, n/k ≈ n)
	
Masters theorm
--------------
If T(n) = aT(n-b) + f(n) 
	where a > 0, b > 0 and 
	f(n) = O(n<sup>k</sup>) where k > 0, 
then time complexity 
	O(nf(n)) or O(n<sup>k+1</sup>) if a = 1
	O(a<sup>n</sup>f(n)) or O(a<sup>n</sup>n<sup>k</sup>) if a > 1
	O(f(n)) or O(n<sup>k</sup>) if a < 1

	