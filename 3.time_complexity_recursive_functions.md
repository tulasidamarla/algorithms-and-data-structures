Divide and Conquer
------------------
A divide-and-conquer algorithm is recursively breaking down a problem into two or more sub-problems of the same or related type, until these become simple enough to be solved directly.

The strategy is :

		dac(p){
			if(small(p)){
				solution(p);
			} else {
				divide p into p1,p2,p3.....pk
				apply dac(p1),dac(p2)....apply(dac(pk))
				combine(dac(p1),dac(p2),...dac(pk));
			}
		}

Common problems that can be solved using Divide and Conquer approach are
- Binary Search
- Find max and min
- Merge Sort
- Quick Sort
- Strassen's matrix multiplication

Divide and Conquer strategy is recursive. For this we should know how to find time complexity of recursive functions. For this we use <b>Recurrence Relation</b>

Recurrence Relation for Decreasing functions
---------------------------------------------
Problem 1
---------

		void test(n){
			if( n > 0) { // constant time c0
				System.out.println(n); // constant time c1
				test(n-1); // T(n-1)
			}
		}

T(n) = T(n-1) + (c0 + c1) 
<ins>T(n) = T(n-1) + 1 </ins> // we consider asymptotic notation. so c0 + c1 can be written as 1.

Finding Time complexity
-----------------------

Terminating condition for algorithm is n > 0, it means when n = 0, T(n) = 1.

		T(n) =  1 			when n = 0
		T(n) = T(n-1) + 1 	when n > 0
		
Solution:
		
		T(n) = T(n-1) + 1
		T(n) = T(n-2) + 2 // substitute the formula T(n-1) = T(n-2) + 1
		T(n) = T(n-3) + 3 // substitute the formula T(n-2) = T(n-3) + 1
		....
		T(n) = T(n-k) + k

		T(n) = T(n-n) + n // when k = n

So, time complexity of the problem is O(n).

Problem 2
---------

		void test(n){
			if( n > 0) { // constant time c0
				for(int i = 0; i < n ; i++){ // n times
					System.out.println(n); // constant time c1 * n
				}
				test(n-1); // T(n-1)
			}
		}

T(n) = T(n-1) + n + c<sub>1</sub>n + c<sub>0</sub>
T(n) = T(n-1) + (1 + c<sub>1</sub>)n + c<sub>1</sub>

<b>T(n) = T(n-1) + n </b> // Assympotatic notation for the above (1 + c<sub>1</sub>)n + c<sub>0</sub> becomes n.

Finding Time complexity
-----------------------

Terminating condition for algorithm is n > 0, it means when n = 0, T(n) = 1.

		T(n) =  1 			when n = 0
		T(n) = T(n-1) + n 	when n > 0
		
Solution:
		
		T(n) = T(n-1) + n
		T(n) = T(n-2) + (n-1) + n // substitute the formula T(n-1) = T(n-2) + (n-1)
		T(n) = T(n-3) + (n-2) + (n-1) + n // substitute the formula T(n-2) = T(n-3) + (n-2)
		....
		T(n) = T(n-k) + (n-k+1) + (n-k+2) + ....(n-2) + (n-1) + n

		T(n) = T(n-n) + 1 + 2 + 3 + ..... + (n-2) + (n-1) + n // when k = n
		
		T(n) = n(n+1)/2

So, time complexity of the problem is O(n<sup>2</sup>).

Problem 3
---------

		void test(n){
			if( n > 0) { // constant time c0
				for(int i = 0; i < n ; i*2){ // log(n) times
					System.out.println(n); // constant time c1 * log(n)
				}
				test(n-1); // T(n-1)
			}
		}

T(n) = T(n-1) + log(n) + c<sub>1</sub>log(n) + c<sub>0</sub>
T(n) = T(n-1) + (c<sub>0</sub> + c<sub>1</sub>)log(n) + c<sub>0</sub>

<ins>T(n) = T(n-1) + log(n) </ins> // Assympotatic notation for the above (c<sub>0</sub> + c<sub>1</sub>)log(n) + c<sub>0</sub> becomes log(n).

Finding Time complexity
-----------------------

Terminating condition for algorithm is n > 0, it means when n = 0, T(n) = 1.

		T(n) =  1 			when n = 0
		T(n) = T(n-1) + log(n) 	when n > 0
		
Solution:
		
		T(n) = T(n-1) + log(n)
		T(n) = T(n-2) + log(n-1) + log(n) // substitute the formula T(n-1) = T(n-2) + log(n-1)
		T(n) = T(n-3) + log(n-2) + log(n-1) + log(n) // substitute the formula T(n-2) = T(n-3) + log(n-2)
		....
		T(n) = T(n-k) + log(n-k+1) + log(n-k+2) + ....log(n-3) + log(n-2) + log(n-1) + log(n)

		T(n) = T(n-n) + log(1) + log(2) + log(3)....log(n-3) + log(n-2) + log(n-1) + log(n) // when k = n
		
		T(n) = 1 + log(1*2*3*...n) // using the formula log(a) + log(b) = log(ab)
		T(n) = 1 + log(n!) // The next higher order term of n! is n<sup>n</sup>
		T(n) = 1 + nlog(n)
		

So, time complexity of the problem is O(nlog(n)).
		
Problem 4
---------
		
		void test(n){
			if( n > 0) { // constant time c0
				System.out.println(n); // constant time c1 * log(n)
				test(n-1); // T(n-1)
				test(n-1); // T(n-1)
			}
		}

T(n) = 2T(n-1) + c<sub>1</sub> + c<sub>0</sub>

<ins>T(n) = 2T(n-1) + 1 // Assympotatic notation for the above c<sub>1</sub> + c<sub>0</sub> </ins> becomes 1.

Finding Time complexity
-----------------------

Terminating condition for algorithm is n > 0, it means when n = 0, T(n) = 1.

		T(n) =  1 			when n = 0
		T(n) = T(n-1) + log(n) 	when n > 0
		
Solution:
		
		T(n) = 2T(n-1) + 1
		T(n) = 2^2T(n-2) + 2 + 1 // substitute the formula T(n-1) = T(n-2) + log(n-1)
		T(n) = 2^3T(n-3) + 2^2 + 2 + 1 // substitute the formula T(n-2) = T(n-3) + log(n-2)
		....
		T(n) = 2^kT(n-k) + 2^(k-1) + 2^(k-2) + ....2^3 + 2^2 + 2^1 + 2^0

		T(n) = 2^n + 2^(n-1) + 2^(n-2) + ....2^3 + 2^2 + 2^1 + 2^0  // when k = n
		
		T(n) = 2^(n+1)-1 // using the formula a + ar^2 + ar^3 + ...ar^n-1 = a(r^n-1)/(r-1) where r = 2 and a =1 in this scenario

So, time complexity of the problem is O(2<sup>n</sup>).

Deriving masters theorm from the above solved problems
------------------------------------------------------

		T(n) = T(n-1) + 1		->	O(n)
		T(n) = T(n-1) + n		->	O(n^2)
		T(n) = T(n-1) + log(n)	->	O(nlog(n))
		T(n) = 2T(n-1)+ 1		-> 	O(2^n)
		T(n) = 3T(n-1)+ 1		-> 	O(3^n)	
		T(n) = 2T(n-1)+ n		-> 	O(n2^n)
Based on the above 3 problems we have solved already, we can find time complexity for the following problem:
		
		T(n) = T(n-2) + 1	->	n/2 (decrease by 2 means n/2) ≈ O(n)
		T(n) = T(n-100) + 1	->	n/100 ≈ O(n)
		T(n) = T(n-k) + 1 	->  n/k ≈ O(n) (at larger values of n, n/k ≈ n)
	
Masters theorm
--------------
If T(n) = aT(n-b) + f(n) 
	where a > 0, b > 0 and 
	f(n) = O(n<sup>k</sup>) where k > 0, 
then time complexity 
	O(nf(n)) or O(n<sup>k+1</sup>) if a = 1
	O(a<sup>n</sup>f(n)) or O(a<sup>n</sup>n<sup>k</sup>) if a > 1
	O(f(n)) or O(n<sup>k</sup>) if a < 1

Recurrence Relation for Dividing functions
---------------------------------------------
Problem 1
---------

		test(n){
			if(n > 1) { //c0 
				System.out.println(n); // c1
				test(n/2); // n/2 times
			}
		}

T(n) = T(n/2) + (c0 + c1)
 
<b>T(n) = T(n/2) + 1 </b> 

Finding Time complexity
-----------------------

Terminating condition for algorithm is n > 1, it means when n = 1, T(n) = 1.

		T(n) =  1 			when n = 1
		T(n) = T(n/2) + 1 	when n > 1
		
Solution:
		
		T(n) = T(n/2) + 1
		T(n) = T(n/2^2) + 2 // substitute the formula T(n) = T(n/2) + 1
		T(n) = T(n/2^3) + 3 // substitute the formula T(n/2) = T(n/2^2) + 1
		....
		T(n) = T(n/2^k) + k

For the above equation to terminate, n/2^k = 1 i.e. when k = log<sub>2</sub>n.

		T(n) = T(1) + log(n) // when n = 2^k

So, time complexity of the problem is O(log(n)).

Problem 2
---------

		test(n){
			if(n > 1) { //c0 
				
				for(int i = 0; i < n ; i++) {
					System.out.println(n); // c1
				}
				test(n/2); // n/2 times
			}
		}

T(n) = T(n/2) + (c0 + c1*n) 

<b>T(n) = T(n/2) + n </b> 

Finding Time complexity
-----------------------

Terminating condition for algorithm is n > 1, it means when n = 1, T(n) = 1.

		T(n) =  1 			when n = 1
		T(n) = T(n/2) + n 	when n > 1
		
Solution:
		
		T(n) = T(n/2) + n
		T(n) = T(n/2^2) + n/2 + n // substitute the formula T(n) = T(n/2) + n
		T(n) = T(n/2^3) + n/2^2 + n/2 + n // substitute the formula T(n/2^2) = T(2/2^3) + (n/2^2)
		....
		T(n) = T(n/2^k) + n/2^k + n/2^(k-1) + ... + n/2^2 + n/2 + n

For the above equation to terminate, n/2^k = 1 or when k = log<sub>2</sub>n.

		T(n) = T(n/2^k) + n(1 + 1/2 + 1/2^2 + 1/2^3 + ...+1/2^k) 
		T(n) = T(1) + n(1 + 1/2 + 1/2^2 + 1/2^3 + ...+1/n) // substituting n = 2^k
		T(n) = 1 + n(1 + 1) //  1/2 + 1/2^2 + 1/2^3 + .... = 1 (circle partition logic)
		T(n) = 2n + 1
		
So, time complexity of the problem is O(n).

Problem 3
---------

		test(n){
			if(n > 1) { //c0 
				
				for(int i = 0; i < n ; i++) {
					System.out.println(n); // c1
				}
				test(n/2); // n/2 times
				test(n/2);
			}
		}

T(n) = 2T(n/2) + (c0 + c1*n) 

<b>T(n) = 2T(n/2) + n </b> 

Finding Time complexity
-----------------------

Terminating condition for algorithm is n > 1, it means when n = 1, T(n) = 1.

		T(n) =  1 			when n = 1
		T(n) = 2T(n/2) + n 	when n > 1
		
Solution:
		
		T(n) = 2T(n/2) + n
		T(n) = 2^2T(n/2^2) + 2*n/2 + n = 2^2T(n/2^2) + 2n // substitute the formula T(n/2) = 2T(n/2) + n
		T(n) = 2^3T(n/2^3) + 2^2*n/2^2 + 2n = 2^3T(n/2^3) + 3n // substitute the formula T(n/2^2) = 2T(n/2^3) + n/2^2
		....
		T(n) = 2^kT(n/2^k) + k*n
		
For the above equation to terminate, n/2^k = 1 or when k = log<sub>2</sub>n.

		T(n) = 2^log(n)T(1) + nlog(n)) 
		T(n) = 2 + nlog(n)
		
So, time complexity of the problem is O(nlog(n)).

Masters theorm for dividing functions
-------------------------------------
If T(n) = aT(n/b) + f(n) <br>
 - where a >= 1, b > 1 and 
 - f(n) = O(n<sup>k</sup> (logn)<sup>p</sup>) where k > 0, then 
   - Case 1: If log<sub>b</sub>a > k, then<br>
     - time complexity O(n<sup>log<sub>b</sub>a</sup>)
   - Case 2: If log<sub>b</sub>a = k, then<br>
	 - if p > -1 time complexity O(n<sup>k</sup>(logn)<sup>p+1</sup>)
	 - if p = -1 time complexity O(n<sup>k</sup>loglog(n))
	 - if p < -1 time complexity O(n<sup>k</sup>)
   - Case 3: if log<sub>b</sub>a = k, then<br>
	 - if p >=0 time complexity O(n<sup>k</sup>(logn)<sup>p</sup>)
     - if p <0 time complexity O(n<sup>k</sup>)
	
Case 1 examples
---------------
a)T(n) = 2T(n/2) + 1

a = 2, b = 2, k = 0 (1 can be written as n^0) , this belongs to case 1 because (log<sub>2</sub>2 ) > 0 
	
Time complexity : O(n<sup>log<sub>2</sub>2</sup>)	= O(n)

b)T(n) = 4T(n/2) + 1

a = 4, b = 2, k = 0 (log<sub>2</sub>4 > 0)

Time complexity : O(n<sup>log<sub>2</sub>4</sup>)	= O(n<sup>2</sup>)

c)T(n) = 8T(n/2) + 1

Time complexity : O(n<sup>log<sub>2</sub>8</sup>)	= O(n<sup>3</sup>)

d)T(n) = 8T(n/2) + n<sup>2</sup>

a = 8, b = 2, k = 2 , this belongs to case 1 because (log<sub>2</sub>8  > 2) 

e)d)T(n) = 8T(n/2) + nlog<sub>2</sub>n

a = 8, b = 2, k = 1 , this belongs to case 1 because (log<sub>2</sub>8  > 1) 

Time complexity : O(n<sup>log<sub>2</sub>8</sup>)	= O(n<sup>3</sup>)

Note: As long as log of a base b greater than k, it won't consider the log component of the second term.

Conclusion: If a base b of first term greater than power of n of second term, it belongs to case 1.

Case 2 examples
---------------

As we know In case 2, we have 3 subcases p > -1, p =-1 and p< -1. 

p > -1
------
a)T(n) = 2T(n/2) + n

a = 2, b = 2, k = 1 this belongs to case 2 because (log<sub>2</sub>2 = 1). No log component in second term, so p is 0.

Time complexity is : O(n<sup>k</sup>(logn)<sup>p+1</sup>) = O(nlogn)

b)T(n) = 4T(n/2) + n<sup>2</sup>

a = 4, b = 2, k = 2 and p = 0

Time complexity is : O(n<sup>2</sup>logn)

c)T(n) = 4T(n/2) + n<sup>2</sup>logn

Time complexity is : O(n<sup>2</sup>log<sup>2</sup>n)

d)T(n) = 4T(n/2) + n<sup>2</sup>log<sup>2</sup>n

Time complexity is : O(n<sup>2</sup>log<sup>3</sup>n)

Conclusion: If a base b and k are equal then simply write n power k and increase log term power by 1.

p = -1
------
a)T(n) = 2T(n/2) + n/logn

a = 2, b = 2, k = 1 , p = -1

Time complexity is : O(n<sup>k</sup>loglogn) = O(nloglogn)

p <= -1
-------
a)T(n) = 2T(n/2) + n/log<sup>2</sup>n

a = 2, b = 2, k = 1 , p = -2

Time complexity is : O(n<sup>k</sup>) = O(n)

Case 3
------
In case 3, we have two cases.

p >= 0
-----
a)T(n) = T(n/2) + n<sup>2</sup>

a = 2, b = 2, k = 2 , p = 0

Time complexity is : O(n<sup>k</sup>(logn)<sup>p</sup>) = O(n<sup>2</sup>)

b)T(n) = T(n/2) + n<sup>2</sup>logn

a = 2, b = 2, k = 2 , p = 1

Time complexity is : O(n<sup>2</sup>logn)

c)T(n) = T(n/2) + n<sup>2</sup>log<sup>2</sup>n

a = 2, b = 2, k = 2 , p = 2

Time complexity is : O(n<sup>2</sup>log<sup>2</sup>n)

p < 0
-----
a)T(n) = T(n/2) + n<sup>2</sup>/logn

a = 2, b = 2, k = 2 , p = -1

Time complexity is : O(n<sup>k</sup>) = O(n<sup>2</sup>)

Recurrence Relation for Root functions
--------------------------------------
Problem 1
---------

		test(n){
			if(n > 1) { //c0 
				System.out.println(n); // c1
				test(√n); // 
			}
		}

T(n) = T(√n) + (c0 + c1)
 
<b>T(n) = T(√n) + 1 </b>


Finding Time complexity
-----------------------

Terminating condition for algorithm is n = 2. It means when n = 2, T(n) = 1.

		T(n) =  1 	when n = 2 (we consider T(√2) = 1)
		T(n) = T(√n) + 1 	when n > 2

Solution:
		
		T(n) = T(n^1/2) + 1
		T(n) = T(n^1/2^2) + 1 + 1 = T(n^1/2^2) + 2 
		T(n) = T(n^1/2^3) + 1 + 2 = T(n^1/2^3) + 3 
		....
		T(n) = T(n^1/2^k) + k

This will terminate when n<sup>1/2<sup>k</sup></sup> becomes √2. <br>
&nbsp;&nbsp;		n<sup>1/2<sup>k</sup></sup> = 2<sup>1/2</sup><br>
Applying log on both sides,<br>
&nbsp;&nbsp;		logn(1/2<sup>k</sup>) = 1/2(log<sub>2</sub>2)<br>
&nbsp;&nbsp;		logn(1/2<sup>k</sup>) = 1/2<br>
&nbsp;&nbsp;		logn 2<sup>-k</sup> = 2 <sup>-1</sup><br>
&nbsp;&nbsp;		logn = 2<sup>k-1</sup><br>
Again applying log,<br>
&nbsp;&nbsp;		loglogn	= k-1<br>
&nbsp;&nbsp;		k = loglogn

so, time complexity is O(loglogn)		
		

		


